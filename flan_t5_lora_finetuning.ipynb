{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fine-Tuning FLAN-T5 for Dialogue Summarization with LoRA\n",
                "\n",
                "This notebook demonstrates how to fine-tune the `google/flan-t5-base` model for dialogue summarization using the `knkarthick/dialogsum` dataset with Parameter-Efficient Fine-Tuning (PEFT) via LoRA (Low-Rank Adaptation).\n",
                "\n",
                "## Overview\n",
                "- **Model**: google/flan-t5-base\n",
                "- **Dataset**: knkarthick/dialogsum (subset: 500 train / 200 val)\n",
                "- **Technique**: LoRA (Low-Rank Adaptation)\n",
                "- **Task**: Seq2Seq Dialogue Summarization\n",
                "\n",
                "## Key Parameters\n",
                "| Parameter | Value |\n",
                "|-----------|-------|\n",
                "| LoRA Rank (r) | 16 |\n",
                "| LoRA Alpha | 32 |\n",
                "| Dropout | 0.05 |\n",
                "| Target Modules | q, v |\n",
                "| Learning Rate | 1e-3 |\n",
                "| Epochs | 1 |\n",
                "| Batch Size | 2 (effective 8 via grad accum) |\n",
                "| Train Subset | 500 samples |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Install Required Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
                        "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
                    ]
                }
            ],
            "source": [
                "!pip install transformers datasets peft evaluate rouge_score accelerate -q"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\CHANUPA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "WARNING:tensorflow:From c:\\Users\\CHANUPA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import evaluate\n",
                "import numpy as np\n",
                "import warnings\n",
                "from datasets import load_dataset\n",
                "from peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig\n",
                "from transformers import (\n",
                "    AutoModelForSeq2SeqLM,\n",
                "    AutoTokenizer,\n",
                "    DataCollatorForSeq2Seq,\n",
                "    Seq2SeqTrainingArguments,\n",
                "    Seq2SeqTrainer\n",
                ")\n",
                "\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cpu\n"
                    ]
                }
            ],
            "source": [
                "MODEL_ID = \"google/flan-t5-base\"\n",
                "DATASET_ID = \"knkarthick/dialogsum\"\n",
                "OUTPUT_DIR = \"./flan-t5-lora-dialogue-summary\"\n",
                "\n",
                "MAX_INPUT_LENGTH = 512       # reduced from 1024 — saves memory & speeds tokenization\n",
                "MAX_TARGET_LENGTH = 128\n",
                "\n",
                "LORA_R = 16\n",
                "LORA_ALPHA = 32\n",
                "LORA_DROPOUT = 0.05\n",
                "TARGET_MODULES = [\"q\", \"v\"]\n",
                "\n",
                "LEARNING_RATE = 1e-3\n",
                "NUM_EPOCHS = 1               # reduced from 4 — single pass is enough for a quick test\n",
                "BATCH_SIZE = 8\n",
                "GRAD_ACCUM_STEPS = 1\n",
                "TRAIN_SUBSET_SIZE = 1000\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load Tokenizer and Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "`torch_dtype` is deprecated! Use `dtype` instead!\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model parameters: 247,577,856\n"
                    ]
                }
            ],
            "source": [
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
                "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID, torch_dtype=torch.float32)\n",
                "\n",
                "print(f\"Model parameters: {model.num_parameters():,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "DatasetDict({\n",
                        "    train: Dataset({\n",
                        "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
                        "        num_rows: 12460\n",
                        "    })\n",
                        "    validation: Dataset({\n",
                        "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
                        "        num_rows: 500\n",
                        "    })\n",
                        "    test: Dataset({\n",
                        "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
                        "        num_rows: 1500\n",
                        "    })\n",
                        "})\n"
                    ]
                }
            ],
            "source": [
                "dataset = load_dataset(DATASET_ID)\n",
                "print(dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dialogue:\n",
                        "#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n",
                        "#Person2#: I found it would be a good idea to get a check-up.\n",
                        "#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n",
                        "#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n",
                        "#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n",
                        "#Person2#: Ok.\n",
                        "#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n",
                        "#Person2#: Yes.\n",
                        "#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n",
                        "#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n",
                        "#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n",
                        "#Person2#: Ok, thanks doctor.\n",
                        "\n",
                        "Summary:\n",
                        "Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\n"
                    ]
                }
            ],
            "source": [
                "sample = dataset[\"train\"][0]\n",
                "print(f\"Dialogue:\\n{sample['dialogue']}\\n\")\n",
                "print(f\"Summary:\\n{sample['summary']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Data Preprocessing and Tokenization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess_function(examples):\n",
                "    inputs = [\n",
                "        f\"Summarize the following conversation:\\n\\n{dialogue}\\n\\nSummary:\" \n",
                "        for dialogue in examples[\"dialogue\"]\n",
                "    ]\n",
                "    \n",
                "    model_inputs = tokenizer(\n",
                "        inputs, \n",
                "        max_length=MAX_INPUT_LENGTH, \n",
                "        truncation=True, \n",
                "        padding=False\n",
                "    )\n",
                "    \n",
                "    labels = tokenizer(\n",
                "        text_target=examples[\"summary\"], \n",
                "        max_length=MAX_TARGET_LENGTH, \n",
                "        truncation=True, \n",
                "        padding=False\n",
                "    )\n",
                "    \n",
                "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
                "    return model_inputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Map: 100%|██████████| 500/500 [00:00<00:00, 4464.29 examples/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train size (subset): 1,000\n",
                        "Validation size (subset): 200\n",
                        "Test size: 1,500\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "tokenized_dataset = dataset.map(\n",
                "    preprocess_function, \n",
                "    batched=True, \n",
                "    remove_columns=[\"id\", \"topic\", \"dialogue\", \"summary\"]\n",
                ")\n",
                "\n",
                "# Use a small subset for faster training\n",
                "train_subset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(TRAIN_SUBSET_SIZE))\n",
                "val_subset = tokenized_dataset[\"validation\"].shuffle(seed=42).select(range(200))\n",
                "\n",
                "print(f\"Train size (subset): {len(train_subset):,}\")\n",
                "print(f\"Validation size (subset): {len(val_subset):,}\")\n",
                "print(f\"Test size: {len(tokenized_dataset['test']):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Setup LoRA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "trainable params: 1,769,472 || all params: 249,347,328 || trainable%: 0.7096\n"
                    ]
                }
            ],
            "source": [
                "lora_config = LoraConfig(\n",
                "    r=LORA_R,\n",
                "    lora_alpha=LORA_ALPHA,\n",
                "    target_modules=TARGET_MODULES,\n",
                "    lora_dropout=LORA_DROPOUT,\n",
                "    bias=\"none\",\n",
                "    task_type=TaskType.SEQ_2_SEQ_LM\n",
                ")\n",
                "\n",
                "peft_model = get_peft_model(model, lora_config)\n",
                "peft_model.enable_input_require_grads()  # required for gradient_checkpointing\n",
                "peft_model.print_trainable_parameters()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Define Evaluation Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "rouge = evaluate.load(\"rouge\")\n",
                "\n",
                "def compute_metrics(eval_pred):\n",
                "    predictions, labels = eval_pred\n",
                "\n",
                "    # If predictions are logits (3D), take argmax to get token IDs\n",
                "    if isinstance(predictions, tuple):\n",
                "        predictions = predictions[0]\n",
                "    predictions = np.array(predictions)\n",
                "    if predictions.ndim == 3:\n",
                "        predictions = np.argmax(predictions, axis=-1)\n",
                "\n",
                "    # Clip to valid token ID range to prevent OverflowError\n",
                "    predictions = np.clip(predictions, 0, tokenizer.vocab_size - 1)\n",
                "    # Replace -100 in predictions with pad_token_id\n",
                "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
                "\n",
                "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
                "\n",
                "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
                "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
                "\n",
                "    # Strip whitespace\n",
                "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
                "    decoded_labels = [label.strip() for label in decoded_labels]\n",
                "\n",
                "    result = rouge.compute(\n",
                "        predictions=decoded_preds,\n",
                "        references=decoded_labels,\n",
                "        use_stemmer=True\n",
                "    )\n",
                "\n",
                "    return {k: round(v * 100, 4) for k, v in result.items()}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Configure Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "training_args = Seq2SeqTrainingArguments(\n",
                "    output_dir=OUTPUT_DIR,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    num_train_epochs=NUM_EPOCHS,\n",
                "    per_device_train_batch_size=BATCH_SIZE,\n",
                "    per_device_eval_batch_size=BATCH_SIZE,\n",
                "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
                "    weight_decay=0.01,\n",
                "    save_total_limit=1,\n",
                "    predict_with_generate=True,\n",
                "    generation_max_length=MAX_TARGET_LENGTH,\n",
                "    logging_steps=25,\n",
                "    eval_strategy=\"no\",         # skip evaluation during training for speed\n",
                "    save_strategy=\"epoch\",\n",
                "    report_to=\"none\",\n",
                "    fp16=torch.cuda.is_available(),\n",
                "    gradient_checkpointing=True,\n",
                ")\n",
                "\n",
                "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=peft_model)\n",
                "\n",
                "trainer = Seq2SeqTrainer(\n",
                "    model=peft_model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_subset,         # use subset\n",
                "    eval_dataset=val_subset,            # use subset\n",
                "    tokenizer=tokenizer,\n",
                "    data_collator=data_collator,\n",
                "    compute_metrics=compute_metrics,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Train the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [125/125 49:10, Epoch 1/1]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Step</th>\n",
                            "      <th>Training Loss</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>25</td>\n",
                            "      <td>1.503900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>50</td>\n",
                            "      <td>1.311300</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>75</td>\n",
                            "      <td>1.294800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>100</td>\n",
                            "      <td>1.295200</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>125</td>\n",
                            "      <td>1.400700</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Loss: 1.3612\n"
                    ]
                }
            ],
            "source": [
                "train_result = trainer.train()\n",
                "\n",
                "print(f\"Training Loss: {train_result.training_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Save the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model saved to: ./flan-t5-lora-dialogue-summary\n"
                    ]
                }
            ],
            "source": [
                "peft_model.save_pretrained(OUTPUT_DIR)\n",
                "tokenizer.save_pretrained(OUTPUT_DIR)\n",
                "\n",
                "print(f\"Model saved to: {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Evaluate on Test Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [188/188 24:36]\n",
                            "    </div>\n",
                            "    "
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Test Set Results:\n",
                        "  ROUGE-1:    41.00%\n",
                        "  ROUGE-2:    15.57%\n",
                        "  ROUGE-L:    33.18%\n",
                        "  ROUGE-Lsum: 33.15%\n"
                    ]
                }
            ],
            "source": [
                "def evaluate_on_test_set(trainer, tokenized_dataset):\n",
                "    test_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
                "    \n",
                "    print(\"Test Set Results:\")\n",
                "    print(f\"  ROUGE-1:    {test_results['eval_rouge1']:.2f}%\")\n",
                "    print(f\"  ROUGE-2:    {test_results['eval_rouge2']:.2f}%\")\n",
                "    print(f\"  ROUGE-L:    {test_results['eval_rougeL']:.2f}%\")\n",
                "    print(f\"  ROUGE-Lsum: {test_results['eval_rougeLsum']:.2f}%\")\n",
                "    \n",
                "    return test_results\n",
                "\n",
                "test_results = evaluate_on_test_set(trainer, tokenized_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Inference - Load and Test Saved Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBD0729D90>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 3ffa449f-0a67-4aa0-a356-776d7f99125e)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
                        "WARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBD0729D90>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 3ffa449f-0a67-4aa0-a356-776d7f99125e)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
                        "Retrying in 1s [Retry 1/5].\n",
                        "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
                        "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EB86E56360>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 5291a991-9159-44e7-93c2-11010db2850f)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
                        "WARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EB86E56360>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 5291a991-9159-44e7-93c2-11010db2850f)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
                        "Retrying in 2s [Retry 2/5].\n",
                        "WARNING:huggingface_hub.utils._http:Retrying in 2s [Retry 2/5].\n",
                        "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBD1FD6ED0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 2aa1be67-2d31-4195-8e7d-a9cca2514573)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
                        "WARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBD1FD6ED0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 2aa1be67-2d31-4195-8e7d-a9cca2514573)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
                        "Retrying in 4s [Retry 3/5].\n",
                        "WARNING:huggingface_hub.utils._http:Retrying in 4s [Retry 3/5].\n",
                        "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF06EED0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: b0394663-edae-46b5-8186-b966fd28b72e)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
                        "WARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF06EED0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: b0394663-edae-46b5-8186-b966fd28b72e)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
                        "Retrying in 8s [Retry 4/5].\n",
                        "WARNING:huggingface_hub.utils._http:Retrying in 8s [Retry 4/5].\n",
                        "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF06FB30>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: f4d3370b-bfc8-4816-bad9-062b0060fb28)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
                        "WARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF06FB30>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: f4d3370b-bfc8-4816-bad9-062b0060fb28)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
                        "Retrying in 8s [Retry 5/5].\n",
                        "WARNING:huggingface_hub.utils._http:Retrying in 8s [Retry 5/5].\n",
                        "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF06F2F0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 324022b0-ae32-4c8b-b2f0-414095137918)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
                        "WARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF06F2F0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 324022b0-ae32-4c8b-b2f0-414095137918)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
                        "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF0818B0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: e811d87b-d543-4564-8fd9-503da3b9039c)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/generation_config.json\n",
                        "WARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF0818B0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: e811d87b-d543-4564-8fd9-503da3b9039c)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/generation_config.json\n",
                        "Retrying in 1s [Retry 1/5].\n",
                        "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
                        "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF080590>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 12a802a3-ab99-4e4b-886e-0af28959863b)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/generation_config.json\n",
                        "WARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF080590>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 12a802a3-ab99-4e4b-886e-0af28959863b)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/generation_config.json\n",
                        "Retrying in 2s [Retry 2/5].\n",
                        "WARNING:huggingface_hub.utils._http:Retrying in 2s [Retry 2/5].\n",
                        "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF082900>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 2bc61454-ef13-4700-98ea-2068818718a3)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/generation_config.json\n",
                        "WARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF082900>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 2bc61454-ef13-4700-98ea-2068818718a3)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/generation_config.json\n",
                        "Retrying in 4s [Retry 3/5].\n",
                        "WARNING:huggingface_hub.utils._http:Retrying in 4s [Retry 3/5].\n",
                        "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF076150>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 0d646b1e-abbd-4470-b3ba-e0dff649f5b4)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/generation_config.json\n",
                        "WARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF076150>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 0d646b1e-abbd-4470-b3ba-e0dff649f5b4)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/generation_config.json\n",
                        "Retrying in 8s [Retry 4/5].\n",
                        "WARNING:huggingface_hub.utils._http:Retrying in 8s [Retry 4/5].\n",
                        "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF076750>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: d07cb813-a6b4-4161-9dc6-93a09758464a)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/generation_config.json\n",
                        "WARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF076750>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: d07cb813-a6b4-4161-9dc6-93a09758464a)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/generation_config.json\n",
                        "Retrying in 8s [Retry 5/5].\n",
                        "WARNING:huggingface_hub.utils._http:Retrying in 8s [Retry 5/5].\n",
                        "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF084170>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 7e25e495-c286-4a8b-8e76-067f580367f0)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/generation_config.json\n",
                        "WARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF084170>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 7e25e495-c286-4a8b-8e76-067f580367f0)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/generation_config.json\n",
                        "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/custom_generate/generate.py (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF085130>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 91169924-6e76-4532-b335-4a1c8bf0bcc6)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/custom_generate/generate.py\n",
                        "WARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/custom_generate/generate.py (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF085130>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 91169924-6e76-4532-b335-4a1c8bf0bcc6)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/custom_generate/generate.py\n",
                        "Retrying in 1s [Retry 1/5].\n",
                        "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
                        "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/custom_generate/generate.py (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF085490>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 9f7d3853-4c29-488d-bd3a-9095198956c3)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/custom_generate/generate.py\n",
                        "WARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/custom_generate/generate.py (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF085490>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 9f7d3853-4c29-488d-bd3a-9095198956c3)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/custom_generate/generate.py\n",
                        "Retrying in 2s [Retry 2/5].\n",
                        "WARNING:huggingface_hub.utils._http:Retrying in 2s [Retry 2/5].\n",
                        "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/custom_generate/generate.py (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF0859D0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: b6ea7f00-2b32-41fb-b858-35a8596f01d0)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/custom_generate/generate.py\n",
                        "WARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/custom_generate/generate.py (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF0859D0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: b6ea7f00-2b32-41fb-b858-35a8596f01d0)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/custom_generate/generate.py\n",
                        "Retrying in 4s [Retry 3/5].\n",
                        "WARNING:huggingface_hub.utils._http:Retrying in 4s [Retry 3/5].\n",
                        "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/custom_generate/generate.py (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF083CB0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 4d5cc755-f0b4-463d-9cb7-1809a828207b)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/custom_generate/generate.py\n",
                        "WARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/custom_generate/generate.py (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF083CB0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 4d5cc755-f0b4-463d-9cb7-1809a828207b)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/custom_generate/generate.py\n",
                        "Retrying in 8s [Retry 4/5].\n",
                        "WARNING:huggingface_hub.utils._http:Retrying in 8s [Retry 4/5].\n",
                        "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/custom_generate/generate.py (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF0838F0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: ac887ab8-7219-480f-9e22-204df0102b62)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/custom_generate/generate.py\n",
                        "WARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/custom_generate/generate.py (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EBDF0838F0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: ac887ab8-7219-480f-9e22-204df0102b62)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/custom_generate/generate.py\n",
                        "Retrying in 8s [Retry 5/5].\n",
                        "WARNING:huggingface_hub.utils._http:Retrying in 8s [Retry 5/5].\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model loaded on cpu\n"
                    ]
                }
            ],
            "source": [
                "config = PeftConfig.from_pretrained(OUTPUT_DIR)\n",
                "\n",
                "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
                "    config.base_model_name_or_path, \n",
                "    torch_dtype=torch.float32\n",
                ")\n",
                "\n",
                "inference_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
                "inference_model.to(device)\n",
                "inference_model.eval()\n",
                "\n",
                "print(f\"Model loaded on {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_summary(model, tokenizer, dialogue, device):\n",
                "    input_text = f\"Summarize the following conversation:\\n\\n{dialogue}\\n\\nSummary:\"\n",
                "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True).to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            input_ids=inputs[\"input_ids\"],\n",
                "            attention_mask=inputs[\"attention_mask\"],\n",
                "            max_new_tokens=128,\n",
                "            do_sample=True,\n",
                "            top_p=0.9,\n",
                "            temperature=0.7\n",
                "        )\n",
                "    \n",
                "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    return summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Input Dialogue:\n",
                        "\n",
                        "#Person1#: I'm thinking of upgrading my computer hardware.\n",
                        "#Person2#: What kind of upgrades do you have in mind?\n",
                        "#Person1#: I definitely need more RAM and maybe a better graphics card for gaming.\n",
                        "#Person2#: Make sure your power supply can handle the new card.\n",
                        "\n",
                        "\n",
                        "Generated Summary:\n",
                        "#Person1# thinks about upgrading his computer hardware. #Person1# needs more RAM and maybe a better graphics card for gaming.\n"
                    ]
                }
            ],
            "source": [
                "sample_dialogue = \"\"\"\n",
                "#Person1#: I'm thinking of upgrading my computer hardware.\n",
                "#Person2#: What kind of upgrades do you have in mind?\n",
                "#Person1#: I definitely need more RAM and maybe a better graphics card for gaming.\n",
                "#Person2#: Make sure your power supply can handle the new card.\n",
                "\"\"\"\n",
                "\n",
                "print(\"Input Dialogue:\")\n",
                "print(sample_dialogue)\n",
                "print(\"\\nGenerated Summary:\")\n",
                "print(generate_summary(inference_model, tokenizer, sample_dialogue, device))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dialogue:\n",
                        "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
                        "#Person2#: Yes, sir...\n",
                        "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
                        "#Person2#: Yes, sir. Go ahead.\n",
                        "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
                        "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
                        "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
                        "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
                        "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
                        "#Person2#: This applies to internal and external communications.\n",
                        "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
                        "#Person2#: Is that all?\n",
                        "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
                        "\n",
                        "Ground Truth Summary:\n",
                        "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
                        "\n",
                        "Generated Summary:\n",
                        "#Person1# needs Ms. Dawson to take a dictation for Ms. Dawson to warn all employees about the new policy and also tells Ms. Dawson about the consequences.\n"
                    ]
                }
            ],
            "source": [
                "test_sample = dataset[\"test\"][0]\n",
                "\n",
                "print(\"Dialogue:\")\n",
                "print(test_sample[\"dialogue\"])\n",
                "print(\"\\nGround Truth Summary:\")\n",
                "print(test_sample[\"summary\"])\n",
                "print(\"\\nGenerated Summary:\")\n",
                "print(generate_summary(inference_model, tokenizer, test_sample[\"dialogue\"], device))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 14. Batch Evaluation Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_samples(model, tokenizer, dataset, device, num_samples=10):\n",
                "    predictions = []\n",
                "    references = []\n",
                "    \n",
                "    for i in range(min(num_samples, len(dataset))):\n",
                "        sample = dataset[i]\n",
                "        generated = generate_summary(model, tokenizer, sample[\"dialogue\"], device)\n",
                "        predictions.append(generated)\n",
                "        references.append(sample[\"summary\"])\n",
                "    \n",
                "    rouge = evaluate.load(\"rouge\")\n",
                "    results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
                "    \n",
                "    print(f\"ROUGE Scores ({num_samples} samples):\")\n",
                "    print(f\"  ROUGE-1: {results['rouge1']*100:.2f}%\")\n",
                "    print(f\"  ROUGE-2: {results['rouge2']*100:.2f}%\")\n",
                "    print(f\"  ROUGE-L: {results['rougeL']*100:.2f}%\")\n",
                "    \n",
                "    return results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 15. Interactive Test — Enter Your Own Conversation\n",
                "\n",
                "Enter a conversation below and the model will generate a summary for it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "INPUT CONVERSATION:\n",
                        "============================================================\n",
                        "\n",
                        "#Person1#: Hey, have you finished the project report yet?\n",
                        "#Person2#: Almost. I just need to add the conclusion and proofread it.\n",
                        "#Person1#: The deadline is tomorrow morning, right?\n",
                        "#Person2#: Yes, I'll have it done by tonight. Can you review it after?\n",
                        "#Person1#: Sure, just send it over when you're done.\n",
                        "#Person2#: Great, I'll email it to you by 9 PM.\n",
                        "\n",
                        "============================================================\n",
                        "MODEL-GENERATED SUMMARY:\n",
                        "============================================================\n",
                        "#Person2# finishes the project report and wants to add the conclusion and proofread it. #Person1# will email it to #Person2# by 9 PM.\n"
                    ]
                }
            ],
            "source": [
                "# Enter your conversation here (use #Person1#: and #Person2#: format)\n",
                "conversation = \"\"\"\n",
                "#Person1#: Hey, have you finished the project report yet?\n",
                "#Person2#: Almost. I just need to add the conclusion and proofread it.\n",
                "#Person1#: The deadline is tomorrow morning, right?\n",
                "#Person2#: Yes, I'll have it done by tonight. Can you review it after?\n",
                "#Person1#: Sure, just send it over when you're done.\n",
                "#Person2#: Great, I'll email it to you by 9 PM.\n",
                "\"\"\"\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"INPUT CONVERSATION:\")\n",
                "print(\"=\" * 60)\n",
                "print(conversation)\n",
                "print(\"=\" * 60)\n",
                "print(\"MODEL-GENERATED SUMMARY:\")\n",
                "print(\"=\" * 60)\n",
                "print(generate_summary(inference_model, tokenizer, conversation, device))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary & Deployment\n",
                "\n",
                "### Training\n",
                "This notebook fine-tuned FLAN-T5 with LoRA on a 1000-sample subset for 1 epoch (fast test).\n",
                "\n",
                "### Deployment Steps\n",
                "\n",
                "**Step 1 — Upload model to Hugging Face Hub**\n",
                "```bash\n",
                "pip install huggingface_hub\n",
                "huggingface-cli login\n",
                "# Edit HF_USERNAME in upload_to_hf.py, then:\n",
                "python upload_to_hf.py\n",
                "```\n",
                "\n",
                "**Step 2 — Create a Hugging Face Space**\n",
                "1. Go to https://huggingface.co/new-space\n",
                "2. Choose **Gradio** SDK, name it `flan-t5-dialogue-summarizer`\n",
                "3. Upload the 3 files from `huggingface_space/` folder (`app.py`, `requirements.txt`, `README.md`)\n",
                "4. Edit `MODEL_REPO` in `app.py` to `\"YOUR_HF_USERNAME/flan-t5-dialogue-summarizer\"`\n",
                "5. The Space builds automatically — your app is live!\n",
                "\n",
                "**Step 3 — GitHub Pages**\n",
                "1. Push this project to a GitHub repo\n",
                "2. Go to repo **Settings → Pages → Source → Deploy from branch → `main` → `/docs`**\n",
                "3. Edit `docs/index.html` and replace `YOUR_HF_USERNAME` with your actual username\n",
                "4. Your site is live at `https://YOUR_GH_USERNAME.github.io/REPO_NAME/`"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
